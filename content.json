[{"title":"Spark0.1源码学习之-编程模型和任务调度","date":"2019-11-04T09:59:00.000Z","path":"2019/11/04/Spark0-1-rdd-and-task-schedule/","text":"从大数据的Hello World说起Spark 是一个大数据（分布式）计算框架，我们从大数据计算的hello world （word count）来看一下spark 的基本思想。 Word Count问题，即输入一份文档文件，计算文档中各单词的个数。 这是一个简单的计数问题，不考虑数据量，一种简单的解决方法是利用一个Map结构，各单词作key，各单词出现的次数作为value，逐个处理各单词，对各单词进行计数。 伪代码基本是： 12345resMap = Map[String,Int]for line in file: for word in line.split(' '): resMap[word]++ 当数据量增大之后（比如上G），逐个单词计数会很低效，这时候可以考虑使用多进程将输入数据拆分成多个小的文件，统计过程划分为两个阶段。 1）统计各个文件中各单词的个数；2）汇总各文件的结果。过程如下图所示： 两个阶段的程序，有不同的职责： 第一个阶段的程序，负责读取数据，对数据进行解析（上例中文档解析出单词），产生中间数据； 第二阶段的程序，负责汇总最终结果。 我们将上述两个阶段分别称为map（和上述Map数据结构不一个概念） 和 reduce. Hadoop上述多线程版本的程序，可以拆解成“单词计数相关的逻辑”以及“解决分治/汇总类问题的通用框架”，比如换成求“全国当前人口的平均年龄”，在单词计数中用到的解析单词之类的逻辑用不到了，但多个map分别读取数据进行计算产生中间结果；将中间结果，发送到reduce，这些通用功能是可以复用的。 当数据量进一步增大，上述计算可以扩展到分布式环境中,即从多进程变成多台机器并行处理。 第一个阶段，map的过程由不同的机器完成，将中间结果发送到一台reduce所在的机器。 第二各阶段，reduce的过程和单机类似，事实上如果最终结果数据量很大，用单一的一个reduce将会成为性能瓶颈。 比如上述Word Count问题，我们想象结果是一个超级大的单词表，虽然单词表很大，但每个单词其实是唯一的。可以有多个reduce，每个reduce只处理特定几个单词的计数。 这样又引入一个新的问题，每个map处理的中间结果，要以特定的规则来分发到reduce上，使得相同的单词的中间结果可以分发到同一个reduce上。这个过程称为 shuffle。 Hadoop的本质就是将上述，Map/Reduce模型以及其依赖的shuffle过程等形成框架。 在大规模分布式环境中，除了编程模型，还需要花费大量的工作在资源分配（哪台机器处理map哪台处理reduce）和任务调度上（还有多少map任务没有结束，reduce任务是否可以开始启动了）。 Hadoop v2将Map/Reduce模型（含任务调度）和资源管理拆分成了两个独立部分，独立出来的资源管理模块就是YARN。 说明：Hadoop相关知识不是本位重点，就像从单机程序讲都只是为了更好的理解spark，所以不会对hadoop做过多的讨论 SparkHadoop将解决问题的模式限定在Map和Reduce两个阶段，Spark提供了更灵活的问题模式的支持。 首先，spark对数据操作做了更多形式的抽象，比如对应于Map阶段细分出flapMap用来表示，将一条转成多条数据。 将reduce阶段细分出 groupby 、reduceByKey等阶段。 其次，M/R的两阶段在Spark中扩展成了，对数据一系列类似map、flatMap等转换的序列。 在Spark中对数据的抽象是RDD（Resilient Distributed Datasets)，百度百科对RDD介绍如下： RDD(Resilient Distributed Datasets)，弹性分布式数据集，是分布式内存的一个抽象概念。RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，只能通过在其他RDD执行确定的转换操作（如map、join和group by）而创建，然而这些限制使得实现容错的开销很低。 即我们可以将RDD看成一个链表，链表的每个节点在上一个节点数据的基础上增加了一些操作而生成。计算最终数据时，只需按照链表上的操作顺序对数据进行计算即可， data -&gt;f1(data)-&gt;f2(f1(data))……-&gt;fn(…f1(data)…) 对于word count 问题，spark代码流程如下： RDD的核心思想 对一个RDD进行操作后形成新的RDD； 每个RDD可以定义为 上游数据集及其上的一个操作 RDD{parent,function} ps: 操作分为两个部分，一个是抽象出来通用的操作类型，如对数据逐个操作、过滤、抽样； 另一部分是更为灵活的逻辑，如 对数据逐个操作中的具体操作逻辑。 前一部分，Spark定义成通用的API，后一部分，用户以函数参数的形式传入。 RDD的起源为使用API由外部数据（文件等）创建得到。 RDD的转换由多个称为Transform操作的API组成 ps:及各种操作对应的 RDD子类 考虑并行处理，每个数据集可以表示成多个 split的组合，及多个子数据集，RDD扩展为 一组数据集 及 其上的一个操作 形成新的一组数据集 由称为Action的多个API生成最终数据。 以 alpha-0.1版本 HdfsTest 为例，代码如下： 12345678910111213141516import spark._object HdfsTest &#123; def main(args: Array[String]) &#123; val sc = new SparkContext(args(0), \"HdfsTest\") val file = sc.textFile(args(1)) val mapped = file.map(s =&gt; s.length).cache() for (iter &lt;- 1 to 10) &#123; val start = System.currentTimeMillis() for (x &lt;- mapped) &#123; x + 2 &#125; // println(\"Processing: \" + x) val end = System.currentTimeMillis() println(\"Iteration \" + iter + \" took \" + (end-start) + \" ms\") &#125; &#125;&#125; 上述代码中，变量file 和 mapped都是 RDD实例，其中，file由SparkContext的API直接从外部文件穿件得到；mapped 根据RDD的转换函数生成。代码中的其他的内容这里不比关注。 RDD类定义在alpha-0.1版本中，RDD定义如下： RDD的函数可以分为两类，一类是用户接口算子，包括对数据进行操作的transform算子，如map、filter、reduce等和触发任务执行的Action算子如 count、collect、foreach等；另一类是任务执行时需要的函数，如split、iterator等，子类通过复写这些函数来实现不同的子RDD。 在alpha-0.1版本中RDD实现了以下函数: 执行函数（子类重载） def splits: Array[Split] 获取数据分片 def iterator(split: Split): Iterator[T] 数据分片上的迭代器 ps:每个分片也是一个数据集，需要提供迭代器来遍历。 def preferredLocations(split: Split): Seq[String] 数据分片引用的数据地址 在RDD类中，上述三个函数只有定义并没有实现，在各子类中具体实现，以HdfsTextFile类为例，函数实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@transient val splits_ = inputFormat.getSplits(conf, sc.scheduler.numCores).map(new HdfsSplit(_)).toArrayoverride def splits = splits_.asInstanceOf[Array[Split]]override def iterator(split_in: Split) = new Iterator[String] &#123; val split = split_in.asInstanceOf[HdfsSplit] var reader: RecordReader[LongWritable, Text] = null ConfigureLock.synchronized &#123; val conf = new JobConf() conf.set(\"io.file.buffer.size\", System.getProperty(\"spark.buffer.size\", \"65536\")) val tif = new TextInputFormat() tif.configure(conf) reader = tif.getRecordReader(split.inputSplit.value, conf, Reporter.NULL) &#125; val lineNum = new LongWritable() val text = new Text() var gotNext = false var finished = false override def hasNext: Boolean = &#123; if (!gotNext) &#123; try &#123; finished = !reader.next(lineNum, text) &#125; catch &#123; case eofe: java.io.EOFException =&gt; finished = true &#125; gotNext = true &#125; !finished &#125; override def next: String = &#123; if (!gotNext) finished = !reader.next(lineNum, text) if (finished) throw new java.util.NoSuchElementException(\"end of stream\") gotNext = false text.toString &#125;&#125;override def preferredLocations(split: Split) = &#123; // TODO: Filtering out \"localhost\" in case of file:// URLs split.asInstanceOf[HdfsSplit].inputSplit.value.getLocations().filter(_ != \"localhost\")&#125; Transform算子 def map(f: T =&gt; U):MappedRDD def flatMap(f: T =&gt; Traversable[U]):FlatMappedRDD def filter(f: T =&gt; Boolean):FilteredRDD def aggregateSplit():SplitRDD def cache():CachedRDD def def sample(withReplacement: Boolean, frac: Double,seed: Int):SampledRDD def union(other: RDD[T]):UnionRDD def ++(other: RDD[T]):UnionRDD def cartesian(other: RDD[U]):CartesianRDD 每个Transform算子会产生一个与其相对类型的RDD，如map算子的实现如下： 1def map[U: ClassManifest](f: T =&gt; U) = new MappedRDD(this, sc.clean(f)) Action算子 def foreach(f: T =&gt; Unit):Unit def collect(): Array[T] def toArray(): Array[T] def reduce(f: (T, T) =&gt; T): T def take(num: Int): Array[T] def first: T def count(): Long 每个Action算子会产生任务调度，如collect算子的实现如下： 12345def collect(): Array[T] = &#123; val tasks = splits.map(s =&gt; new CollectTask(this, s)) val results = sc.runTaskObjects(tasks) Array.concat(results: _*)&#125; RDD类体系RDD分为两大类，一类根据外部数据生成，是RDD的起点，在alpha-0.1版本中只有HdfsTextFile 和ParallelArray；另一类是在RDD增加操作产生，如 MappedRDD、FilteredRDD等，该类RDD和transform算子相对应。继承体系如下： alpha-0.1中实现的RDD有： HdfsTextFile(sc: SparkContext, path: String) ParallelArray(sc: SparkContext, data: Seq[T], numSlices: Int) MappedRDD(prev: RDD[T], f: T =&gt; U) 对应map操作 FilteredRDD(prev: RDD[T], f: T =&gt; Boolean) 对应filter操作 FlatMappedRDD(prev: RDD[T], f: T =&gt; Traversable[U]) 对应flatmap操作 SplitRDD(prev: RDD[T]) SampledRDD(prev: RDD[T], withReplacement: Boolean, frac: Double, seed: Int) 对应sample操作 CachedRDD(prev: RDD[T]) 对应cache操作 UnionRDD(sc: SparkContext, rdd1: RDD[T], rdd2: RDD[T]) 把split 合并 CartesianRDD(sc: SparkContext, rdd1: RDD[T], rdd2: RDD[U]) 任务调度如上所述，Action算子会产生任务，并触发任务的提交。下面我们以foreach为例，追踪任务调度流程。 第一步，在action算子（foreach）中，对每个分区(split)生成Task（ForeachTask）实例，并调用sc（SparkContext）中的 runTaskObjects函数来执行任务。 12345def foreach(f: T =&gt; Unit) &#123; val cleanF = sc.clean(f) val tasks = splits.map(s =&gt; new ForeachTask(this, s, cleanF)).toArray sc.runTaskObjects(tasks)&#125; 第二步，在SparkContext的 runTaskObjects函数中，调用 Scheduler实例的 runTasks函数来执行任务。 1234567891011class SparkContext(master: String, frameworkName: String) extends Logging &#123; private[spark] def runTaskObjects[T: ClassManifest](tasks: Seq[Task[T]]) : Array[T] = &#123; logInfo(\"Running \" + tasks.length + \" tasks in parallel\") val start = System.nanoTime val result = scheduler.runTasks(tasks.toArray) logInfo(\"Tasks finished in \" + (System.nanoTime - start) / 1e9 + \" s\") return result &#125;&#125; Scheduler与Task类体系 SparkContextSparkContext有两大职责，一方面管理着spark运行所需的环境，在alpha-0.1中主要是 任务调度器Scheduler；另一方面向用户提供了编程API。主要函数如下： 运行环境 scheduler: Scheduler 任务调度器 def runTasks(tasks: Array[() =&gt; T]): Array[T] 任务执行函数（由rdd的action算子调用） 编程API def textFile(path: String) : HdfsTextFile def parallelize(seq: Seq[T], numSlices: Int):ParallelArray[T] def parallelize(seq: Seq[T]):ParallelArray[T] def accumulator():Accumulator def broadcast(value: T):CentralizedHDFSBroadcast 总结在 alpha-0.1版本中，实现了基本分布式数据模型RDD的类体系和任务调度模型，但这个版本还比较简单，并没有涉及复杂的操作，比如并没有实现涉及到shuffle过程的操作。但该版本对于理解spark的基本思想还是有很大的帮助。 下一篇文章，我们来调试一下spark0.1版本中的编程模型和任务调度系统。 参考文献 Spark-alpha-0.1源码解读：https://www.jianshu.com/p/795302f94fa1 百度百科：https://baike.baidu.com/item/RDD/5840158 spark源码解析alpha-0.1","tags":[{"name":"spark","slug":"spark","permalink":"https://blog.guopengfei.top/tags/spark/"},{"name":"源码","slug":"源码","permalink":"https://blog.guopengfei.top/tags/源码/"}]},{"title":"OKR简介","date":"2019-06-30T03:09:20.000Z","path":"2019/06/30/okr-introduction/","text":"背景可能是我太后知后觉了，感觉突然OKR就来到了身边。最早是两三个月前在上家公司听到的，OKR似乎很神秘，隐约听到管理层在使用OKR来考核各组甚至每个人的工作。后来听说百度也在用，就专门看了一下OKR的介绍，知道它是一个目标管理的方法。最近来到新的公司，公司也在使用OKR，而且在这里OKR不是管理层的专属，每个人都要制定OKR，所以就来深入的理解一下这个工具。 ORK 是Object&amp;KeyResult 两个单词的缩写， 即目标与衡量目标达成的关键结果，是一种目标管理方法。 [wikipedia]对OKR的描述： Objectives and key results (OKR) is a framework for defining and tracking objectives and their outcomes. 提出与流行OKR最早由 OKRs之父 Andy Grove 在Intel （1976）提出，并且在其书High Output Management（1983）中清晰的定义。 后由 John Doerr在1999年左右 引入Google，并由Google将 OKR 发扬光大，给Google带来了很大的收益。 OKR为Google取得很好收益之后，开始在很多类似的科技创业公司中运用，比如 LinkedIn ，Twitter，和 Uber。 OKR传入中国大概是在2013年底，主要是一些从硅谷回国创业的人开始运用到他们的公司，逐渐开始受到IT、互联网、高科技、海外投资人的追捧，并开始流行起来。 下面来看看什么是OKR，及其设定技巧。 设定目标（Object）目标应该是要完成什么，即想要达成什么（What）目标，需要是产出式的，而不是参与式的。 目标设置应遵循SMART原则： Specific：具体的 Measurable：可衡量的 Aspirational：有挑战性的 Relevant：相关联的 Temporal：有时限的 比如，”进行kylin的调研” 是一个不好的目标，因为无法进行衡量，无法衡量效果，自然很难说有挑战性，也没有时限，很容易就做的不了了之了。 而，”kylin知识沉淀及业务落地“，会更可衡量一些，而相应的KE可以拆解为”文档积累“和 ”业务场景落地“。 目标设定技巧 每次设定3-5个为宜，太多会目标不够聚焦 目标应该有挑战性 对于目标的完成应该有一定的压力，需要全力以赴去做才能达到0.7左右。每次考核在0.7为宜，长期太低，说明目标太不务实，偏离实际；而长期太高说明预期太保守，不能激发潜能，可能只是走个形式。 常规性工作不必设定为ORK，如果是长期重要的工作，可以考虑进行目标拆解。 一年时间内尽量不要变，以确保为了实现这相目标而全力以赴努力，一旦变的多了，就会影响士气，也会影响目标的实现。 公司目标的产生是要通过专业的梳理和分析后得出 愿景、使命和目标是解决方向（Where）的问题，即方向问题，到哪去。 首先要梳理公司战略，未来的发展方向、定位； 还要考虑公司的核心优势、产业的发展趋势、国家的宏观政策； 并结合资本的产业方向。 个人的目标应该支持公司目标（直接或间接） 设定关键结果 (KeyResult)关键结果是检验达成目标的标准。 KR不能太多，每个O最多4个KR 结果要可测量，必须有个number，能很容易的给出分值 对于公司和部门每个KR应该有Owner KR不是Todo List，关注的是结果 但每个KR需要落实到TODO上，以保证KR的实现 KR应该是直接支持目标的，只要KR完成了，我们基本可以判定大的Objective就达到了 但是todo实现不了这样的效果，有时候经常是todo全完成了，但是Objective没有实现 要点提示 OKR在公司内要公开，以确保在尽可能大的范围内目标一致，形成合力。 OKR与KPI不能完全相互替代，因为OKR要求大家都去做挑战的事情，那么常规的日常性事务谁来做？在人员素质很高的情况下(如谷歌)常规性的事务大家都自觉处理好了，而在人员素质没达到很高自觉性下，需要使用KPI来补充管理。 目标应该比较稳定，但应该经常（周、月）更新KR完成情况。 OKR书籍 《OKR源于英特尔和谷歌的目标管理利器》作者：保罗 《OKR工作法》作者：克里斯蒂纳.沃特克Christna 《OKR目标关键成果法：盛行硅谷创新公司目标管理方法》 作者：陈镭 参考资料 Wikipedia OKR： https://en.wikipedia.org/wiki/OKR OKR在国内应用的现状： https://zhuanlan.zhihu.com/p/38152061 OKR用于个人成长： https://www.jianshu.com/p/196f38eedd7d The Basics of OKR https://www.slideshare.net/HenrikJanVanderPol/how-to-outperform-anyone-else-introduction-to-okr How Google sets goals: OKRs https://library.gv.com/how-google-sets-goals-okrs-a1f69b0b72c7","tags":[{"name":"OKR","slug":"OKR","permalink":"https://blog.guopengfei.top/tags/OKR/"}]},{"title":"CleanHandBook (LeetCode) summary","date":"2019-05-24T10:33:48.000Z","path":"2019/05/24/leetcode-summary/","text":"本文总结了 CleanHandBook_v1.0.1.pdf 各题目的解题要点： Chapter 1: Array/String(1-16) Two Sum: 在数据中查找和等于特定值的两个数。 Tips: HashTable 遍历数组中每个值x，x为key，x的索引index为value 存入 map中。如果只判断是否存在，则用set就可以了。 Two Sum II – Input array is sorted: 同上题，但数组是已排序的。 Tips：前后两个指针 Ai+Aj&gt; target ，减小j; &lt; target 增大i. Two Sum III – Data structure design: 设计一个支持 find (查找Two Sum) 和 add的数据结构。 Tips：与上题不同的是只判断是否存在两个值，而且可以重复。 用一个hashMap记录值，并且记录值出现的次数。 Valid Palindrome: 判断一个字符串是否是回文 Tips：1）前后两个指针；2）过滤空白字符 Implement strstr(): 是否包含某个子串 ​ Tips：两层循环，暴力解法。 Reverse Words in a String: 翻转句子中的单词。 Tips：使用一个StringBuilder；从后向前遍历句子中的单词(空格分割) Reverse Words in a String II: 同上，前后无空格，不使用额外存储。 Tips：翻转两趟，第一趟把整句所有字符翻转；第二趟，翻转每个单词中的字符。 String to Integer (atoi): 字符串转整形 Tips：负数判断；溢出判断(2147483647)；从高位循环处理 Valid Number: 判断是否是合法数字 Tips：多个部分多个循环单独处理即可。(空格、+/-、数字、小数点、数字、空格) Longest Substring Without Repeating Characters： Tips: 方案一，HashMap 逐步缩小窗口法；方案二，hashMap 记录位置，跳跃缩小窗口法。 Longest Substring with At Most Two Distinct Characters ： Tips: 方案一，三个变量，i,k分别是窗口的边界，j记录上次字符变换的位置。方案二，滑动缩小窗口法。 Missing Ranges： Tips：start -1 ，end+1。 Longest Palindromic Substring： Tips：遍历回文中心，以中心向外扩张，奇偶两种情况。 One Edit Distance： Tips: 分情况处理 Read N Characters Given Read4： Tips:注意文件结束 Read N Characters Given Read4 – Call multiple times： Tips：上次没用完字符的使用 Chapter 2: Math(17-19) Reverse Integer Tips：溢出处理 214748364 Plus One Tips：进位是确定的只会是1，所以从后往前不为9则停止。 Palindrome Number Tips: 先循环找到最高位的位数； Chapter 3: Linked List(20-24) Merge Two Sorted Lists Tips: 基础链表遍历 Add Two Numbers Tips:注意进位 Swap Nodes in Pairs Tips：pre,p,q,r四个指针 Merge K Sorted Linked Lists Tips: 方案一，分治法，每次缩小一半数量；方案二，大小为k的小顶堆。 Copy List with Random Pointer Tips: 关键是怎样找到原始节点的clone节点；方案一，hashMap法，第一遍遍历，创建各clone节点，第二遍，关联clone节点的random指针；方案二，clone节点插入原节点后面法，第一步，创建各clone节点，第二步，关联random指针，第三步分成两个链表。 Chapter 4: Binary Tree(25-32) Validate Binary Search Tree Tips: 方案一，中序遍历递增；方案二，限定节点取值范围，递归传递范围。null。 Maximum Depth of Binary Tree Tips: 递归 Minimum Depth of Binary Tree Tips:递归；注意如果某个子树为空则不参与比较。 Balanced Binary Tree： Tips: 递归，传递子树深度子树本身是否平衡，可以用-1表示不平衡 Convert Sorted Array to Balanced Binary Search Tree Tips:二分法，递归 Convert Sorted List to Balanced Binary Search Tree Tips: 中序遍历，要知道左右子树的节点数，从原始链表中逐个取数。 Binary Tree Maximum Path Sum Tips: 可能是左右子树联通，也可能向父节点延伸。 Binary Tree Upside Down Tips: 当成特殊链表；方案一，自顶向下递归 ，方案二，字底向上 迭代。 Chapter 5: Bit Manipulation(33-34) Single Number： Tips:异或 Single Number II：(出现三次，只有一个出现一次) Tips:方案一， 记录各位1的总个数，模3；方案二，三个变量分别记录 ones,twos,threes. Chapter 6: Misc(35-38) Spiral Matrix Tips: m,n 记录行、列可以移动的步数；r,l记录当前位置；四个方向单独处理 Integer to Roman Tips: IV, IX 等当成独立的字符来处理；从大到小除求商即为该字符的个数。 Roman to Integer Tips: pre如果比当前小，则减去两倍的pre Clone graph Tips: 记录以及遍历的节点；宽度优先或深度优先 Chapter 7: Stack(39-41) Min Stack Tips:push和pop 时注意栈顶元素和min的关系。 Evaluate Reverse Polish Notation Tips:栈基本操作，遇到操作符就去栈中两个数，求值后再压入栈。 Valid Parentheses Tips:栈基本操作，HashMap记录括号关联关系，左括号进栈，遇到右括号则与栈顶比较。 Chapter 8: Dynamic Programming(42-47) Climbing Stairs Tips: 递归、迭代，斐波那契数列 Unique Paths Tips:方案一， 递归 方案二，优化递归，使用momeration技术，方案三，自底向上，迭代。 Unique Paths II Tips: 同上，注意障碍物上路径为0 Maximum Sum Subarray Tips: f(Ai)= Max(f(Ai-1)+Ai,Ai) Maximum Product Subarray ​ Tips: 记录到位置i的最大值和最小值 Coins in a Line Chapter 9: Binary Search(48-50) Search Insert Position Tips:基本二分法 Find Minimum in Sorted Rotated Array Tips: 二分法简单变体 Find Minimum in Rotated Sorted Array II – with duplicates Tips: 如果都相等则只移动一位索引。","tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://blog.guopengfei.top/tags/LeetCode/"},{"name":"算法","slug":"算法","permalink":"https://blog.guopengfei.top/tags/算法/"}]},{"title":"Spark Streaming in Practice（二）——读写kafka之读Kafka","date":"2019-05-21T11:12:39.000Z","path":"2019/05/21/spark-streaming-in-practice-2/","text":"上篇文章我们一起研究了Kafka的特性，kafka Producer API 以及Spark写API的方案，本文我们继续研究Spark 读取Kafka的方案。ps: 读比较复杂所以放在后面研究。同样的我们从Kakfa 的Consumer API开始。 Kafka Consumer APIsKafka 在0.8版本中有两层Consumer API，low-level的”simple”API和 high-level API。简单的理解 simple API 暴露了更多细节(offset)，用户可以做更精细的控制；而High-leve API因为封装了底层细节（offset）使用起来更方便。 Simple API（0.8）Simple API 维护了对单个broker的连接，每次请求都有闭合的响应。该API是无状态的即每次请求都是独立的，因此每次请求都需要指定要读取的offset，用户需要自己维护已经处理的offset信息。 该接口是线程安全的。 应用场景主要为： 想要多次读取一条信息； 只消费一部分消息； 用户来确保 exact once 语义。 而使用该API就需要用户有更多的工作： 用户需要自己维护已经处理的offset信息，以知道每次请求要从哪个offset开始； 用户需要自己指定各partition的leader broker； 用户需要处理leader的变更情况。 API原型如下： 12345678class kafka.javaapi.consumer.SimpleConsumer &#123; /** * Fetch a set of messages from a topic. * * @param request specifies the topic name, topic partition, starting byte offset, maximum bytes to be fetched. * @return a set of fetched messages */ public FetchResponse fetch(request: kafka.javaapi.FetchRequest); 调用样例如下，可以看到请求中需要包含其实的 offset。 12345FetchRequest req = new FetchRequestBuilder() .clientId(clientName) .addFetch(a_topic, a_partition, readOffset, 100000) .build();FetchResponse fetchResponse = consumer.fetch(req); High-Level API(0.8)High-level API，隐藏了brokers 连接情况的细节，并且是有状态的即维护了已经消费的offset的情况。 该API将已消费的offset存储在zookeeper上。ps:会以Consumer group_id作为子目录，即以Consumer Group来消费数据。 API 原型如下，我们看到该API会对每个Topic建立一个KafkaStream（迭代器）： 1234567891011/** * Create a list of message streams of type T for each topic. * * @param topicCountMap a map of (topic, #streams) pair * @param decoder a decoder that converts from Message to T * @return a map of (topic, list of KafkaStream) pairs. * The number of items in the list is #streams. Each stream supports * an iterator over message/metadata pairs. */public &lt;K,V&gt; Map&lt;String, List&lt;KafkaStream&lt;K,V&gt;&gt;&gt; createMessageStreams(Map&lt;String, Integer&gt; topicCountMap, Decoder&lt;K&gt; keyDecoder, Decoder&lt;V&gt; valueDecoder); 样例如下： 该API会将消费的offset存储在zookeeper中，所以需要给出zk的配置： 123456789private static ConsumerConfig createConsumerConfig(String a_zookeeper, String a_groupId) &#123; Properties props = new Properties(); props.put(\"zookeeper.connect\", a_zookeeper); props.put(\"group.id\", a_groupId); props.put(\"zookeeper.session.timeout.ms\", \"400\"); props.put(\"zookeeper.sync.time.ms\", \"200\"); props.put(\"auto.commit.interval.ms\", \"1000\"); return new ConsumerConfig(props); &#125; 第二步即获取KafkaStream 实例： 1234Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;String, Integer&gt;(); topicCountMap.put(topic, new Integer(a_numThreads)); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; consumerMap = consumer.createMessageStreams(topicCountMap); List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; streams = consumerMap.get(topic); 第三步，用户可以根据迭代器逐个消费数据： 1234ConsumerIterator&lt;byte[], byte[]&gt; it = m_stream.iterator(); while (it.hasNext()) System.out.println(\"Thread \" + m_threadNumber + \": \" + new String(it.next().message())); System.out.println(\"Shutting down Thread: \" + m_threadNumber); 完整样例见：0.8 Using the High Level Consumer Consumer API(1.0)Kafka1.0版本中 Consumer API做了较大升级，主要特点如下： 合并了0.8版本中 simple API和High-level API的功能； 维护了一组连接所需broker是的TCP连接； 该API不是线程安全的； 可以根据参数来确定是自动更新已消费的offset还是手动更新。 自动更新（commit）offset： 1234567891011121314Properties props = new Properties();props.put(\"bootstrap.servers\", \"localhost:9092\");props.put(\"group.id\", \"test\");props.put(\"enable.auto.commit\", \"true\");props.put(\"auto.commit.interval.ms\", \"1000\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);consumer.subscribe(Arrays.asList(\"foo\", \"bar\"));while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());&#125; 手动更新（commit）offset： 123456789101112131415161718192021Properties props = new Properties();props.put(\"bootstrap.servers\", \"localhost:9092\");props.put(\"group.id\", \"test\");props.put(\"enable.auto.commit\", \"false\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);consumer.subscribe(Arrays.asList(\"foo\", \"bar\"));final int minBatchSize = 200;List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = new ArrayList&lt;&gt;();while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; buffer.add(record); &#125; if (buffer.size() &gt;= minBatchSize) &#123; insertIntoDb(buffer); consumer.commitSync(); buffer.clear(); &#125;&#125; Spark 读Kakfa0.8与0.10对比note that the 0.8 integration is compatible with later 0.9 and 0.10 brokers, but the 0.10 integration is not compatible with earlier brokers. spark-streaming-kafka-0-8 spark-streaming-kafka-0-10 Broker Version 0.8.2.1 or higher 0.10.0 or higher Api Stability Stable Experimental Language Support Scala, Java, Python Scala, Java Receiver DStream Yes No Direct DStream Yes Yes SSL / TLS Support No Yes Offset Commit Api No Yes Dynamic Topic Subscription No Yes Spark DStream 类体系从上表我们可以看到，对于kafka 0.8 版本，spark有两套读取API，Direct DStream 和 Receiver DStream。而对于kafka1.0 目前只支持 Direct DStream。我们先从DStream的类体系来看一下这两种方式的异同。 DStream：定义了在RDD之上的基础数据流模型。1）定义了和RDD上类似的一组Transform和Action算子，如map、flatMap等；2）定义接口 def getOrCompute(time: Time): Option[RDD[T]] 返回时间区间内的RDD。 InputDStream：只在driver 上启动一个服务或线程来生成RDDs，可以实现该接口。 ReceiverInputDStream：需要在worker上启动receiver来接收数据，需要实现该接口。ps:需要实现def getReceiver(): Receiver接口。 DirectKafkaInputDStream：基于InputDStream 和 Simple(Low-level) kafka Consumer API 实现。 KafkaInputDStream：基于 ReceiverInputDStream 和 High-level kafka Consumer API 实现。 Spark 读Kafka 0.8从上表我们可以看到，对于kafka 0.8 版本，spark有两套读取API，Direct DStream 和 Receiver DStream。 Receiver DStream在 KafkaReceiver 的 onStart() 函数中： Direct DStream Spark 读Kafka 0.10参考文献 0.8 Using the High Level Consumer kakfa0.8 官网文档 0.8.0 SimpleConsumer Example Spark Streaming + Kafka Integration Guide","tags":[{"name":"spark","slug":"spark","permalink":"https://blog.guopengfei.top/tags/spark/"},{"name":"streaming","slug":"streaming","permalink":"https://blog.guopengfei.top/tags/streaming/"},{"name":"实践","slug":"实践","permalink":"https://blog.guopengfei.top/tags/实践/"}]},{"title":"Spark Streaming in Practice（一）——读写kafka之写Kafka","date":"2019-05-21T00:32:49.000Z","path":"2019/05/21/spark-streaming-in-practice/","text":"记录Spark Streaming开发中的一些经验，有些是在当前理解下的个人总结，可能有失偏颇。无特别说明，下文中Spark 指Spark Streaming 应用。spark Streaming支持很多数据源，如file、socket但无疑Kakfa是其中最重要的streaming数据源，本文会对其重点研究研究。这一块基本思路是，首先看一下Kafka提供的基本的数据读写的API，然后再看一下Spark对相关API进一步封装提供了哪些功能。 Kafka 基本概念kafka是一个分布式的消息队列，即可以有生产者和消费者分别发布和订阅消息，kafka对消息进行持久化缓存，以使得生产者和消费者可以异步通信，并支持多个消费者复用消息。就像一个水池，可以有多个上游往水池中注水，也可以有多个下游从水池中取水。 数据记录：每条数据记录由 key、 value和 timestamp组成。 kafka Broker: kafka由一到多个server组成，每个server称为Broker；每台机器上可以有一到多个broker，一组broker由zookeeper连接起来组成一个kafka集群。 Client(consumer或producer)和server(broker)通过TCP协议通信。 Topic：kafka定义Topic的概念来提升消息队列的并行度，即从应用上可以认为每个Topic是一个单独的消息队列。 Partition：像大多数分布式系统一样，kafka引入Partition(分区)来通过并行执行来提升性能（吞吐等），每个Topic会划分为多个partition，不同的partition分配给不同的broker来处理，所以Broker和Partition是多对多的关系。Produer 可以决定消息的分区规则，来分配到指定的partition。 注意：consumer的数量不能比partitions多（相同的consumer group），最多只会有partitions个consumer消费数据。 Replica（副本）: 像大多数分布式系统一样，kafka引入多副本(Replica)来提升系统容错性，即每个partition会有多个副本，遇到故障时保证还有备份数据可用。每个partition有一个broker作为leader还有0到多个broker作为follower。此处，kafka采用冷备机制，即leader提供数据的读写，follower只是被动的备份数据，只在leader出现故障时，从多个follower中选出新的leader。注意：因为对于不同的broker可以是不同的partition的leader，所以从整体上达到负载均衡的作用，即每个broker都有成为leader提供数据读写的机会 offset：Partition中的每条Message由offset来表示它在这个partition中的偏移量，这个offset不是该Message在partition数据文件中的实际存储位置，而是逻辑上一个值，它唯一确定了partition中的一条Message。因此，可以认为offset是partition中Message的id。实际上，offset是kafka中唯一的元数据。 关于数据顺序：kafka只能保证数据在单个partition内是有序的。若只有一个consumer，则能保证Topic级别的有序。 持久化：kafka会按照设定的生命期缓存所有的消息，其使用文件存储消息(append only log)，为了减少磁盘写入的次数，broker会将消息暂时缓存在内存中，当消息的个数(或尺寸)达到一定阀值时，再flush到磁盘。 Segment：为了提升消息随机读的性能，kafka将数据文件分割为较小的segment。并通过索引来提升数据查询的性能，索引文件记录了offset对应的文件及具体位置。为了减少存储，offset存储的是相对于第一个offset的相对便宜，并没有存储所有offset的索引，而是有一定间隔的稀疏索引。 Consumer Group：在kafka中实际的订阅单位其实是consumer group，即每个Topic中的每条记录只会被一个consumer group消费一次。ps:同一个group中的多个consumer可以在不同的机器上。 Kafka 0.8 Vs. 0.100.10 adds the following over 0.8 Zookeeper connections are discouraged . From 0.10 there won’t be any zookeeper connections required . All connections for consuming data will be maintained by consumer API new unified Consumer API reduced client dependence on zookeeper (offsets stored in Kafka topic) Kafka Streams API Kafka Connect API transport encryption using TLS/SSL Kerberos/SASL Authentication support Access Control Lists timestamps on messages client interceptors lots and lots of bug fixes and improvements 从上，我们可以看出，0.10版本在0.8版本的基础上除了扩充功能，主要是对consumer API的重构。 Kakfa Producer APIProducer API有如下特性： 有同步和异步两种数据处理方式，用配置producer.type=async/sync来区分，默认是sync。其中异步方式会将数据进行缓存，直到达到缓存时间阈值或batch大小阈值才会进行发送； 用户自定义数据分区函数； 用户自定义数据序列化接口； 在Kafka 0.8.0 官网文档上说 可以使用zookeeper来支持 broker的发现，但是在0.8.0 Producer Example 文档中，又推荐使用的是metadata.broker.list 参数。 经测试验证，应该使用metadata.broker.list 否则会报缺少参数。 配置的时候不需要配置所有的brokerlist，只需要配置两个(容错)就可以了，kafka会自动做负载均衡，选出合适的leader。 Spark 写Kafkaspark streaming/spark写Kafka的一个可行方法是，对每个executor 调用创建Kafka Producer 客户端，各分区单独发送数据，样例大体结构如下： 12345result.foreachRDD&#123;rdd=&gt; rdd.foreachPartition&#123; iter=&gt; // 调用kafka Producer 发送数据 &#125;&#125; 但需要注意Producer 实例尽量复用，以提升性能。 Cloudera封装好了一个工具帮我们完成该过程，(原始代码已经找不到，是否复用Producer实例未验证)，该工具其实是从 Jira-4-22 拆分出来的。Maven坐标如下： 123&lt;groupId&gt;org.cloudera.spark.streaming.kafka&lt;/groupId&gt;&lt;artifactId&gt;spark-kafka-writer&lt;/artifactId&gt;&lt;version&gt;0.1.0&lt;/version&gt; 从MavenPository 中看到，最新的版本还是0.1.0，更新日期是 2015年，不确定是有更好的方案还是什么原因，造成该工具一直没有升级。 源码：https://github.com/harishreedharan/spark-streaming-kafka-output ps: 该项目描述中标注了 “Move to org.cloudera “，所以这很大的可能性是该工具的源码。从代码可以看到，其实现跟上述分析的方案一致。 如下代码样例，该工具封装了隐式转换，导入后DStream类型会添加 writeToKafka 函数，并且用户自己添加自定义的分区函数。 1234import org.cloudera.spark.streaming.kafka.KafkaWriter._stream.writeToKafka(producerConf, (x: String) =&gt; new KeyedMessage[String, String](topic,DigestUtils.md5Hex(x), x)) kafka的config会透传下去，到kafka原生Producer API。 参考资料 Kafka：架构简介【转】 Kafka Topic Partition Replica Assignment实现原理及资源隔离方案 Kafka 0.8.0 官网文档 0.8.0 Producer Example","tags":[{"name":"spark","slug":"spark","permalink":"https://blog.guopengfei.top/tags/spark/"},{"name":"streaming","slug":"streaming","permalink":"https://blog.guopengfei.top/tags/streaming/"},{"name":"实践","slug":"实践","permalink":"https://blog.guopengfei.top/tags/实践/"}]},{"title":"Spark存储体系详解","date":"2019-05-15T02:08:16.000Z","path":"2019/05/15/Spark存储体系详解/","text":"存储体系的职责在研究Spark存储体系之前，我们先搞清楚一个重要的问题：对于一个数据计算引擎，存储体系在其中的职责是什么？或者说功能定位是什么？ 而子模块肯定是为整个系统服务的，所以我们可以从 计算引擎(Spark)本身功能来一探究竟。 一个数据计算引擎，最主要的是对数据进行各种加工（转换、过滤、聚合、合并、统计等），我们可以将对数据的加工当成数据状态的转换。所以计算引擎的核心功能就是 定义数据状态 及定义数据状态之上的各种操作，对应于Spark即是，RDD 及之上的各种Transform和Action操作，所以Spark存储体系应该包括对RDD的存储。 输入是外部系统不算Spark本身的存储体系，Spark执行时会将整个作业按照数据依赖情况构建成DAG划分成多个Stage，Stage间涉及到数据的生成和传输(Shuffle阶段) 此处涉及到存储体系，涉及到的功能有数据存储、数据寻址，数据读取和数据远程传输。 在数据处理中，涉及到一个数据对应多路处理的情况，无论用户是否显式的调用Cache，这都涉及到RDD复用，即涉及到中间结果的存储，此处会用到存储体系，涉及到的功能也是数据的存储、寻址和读取。 Spark 提供了Broadcast功能可以将小的数据集同步到多个节点上，该功能也涉及数据的存储、寻址、读取和远程传输。 Spark存储体系主要功能根据以上分析，我们知道Spark存储体系主要功能为： 资源的申请：对于磁盘即创建目录、文件；对于内存即变量的创建、销毁。磁盘资源相对比较丰富，在需要时再去创建目录、文件是完全没问题的，所以不需要太复杂的资源管理。而内存资源是很稀缺的，如果在需要时再去申请很可能出现系统没有足够的内存分配的情况，而如果不加节制的申请也可能自己把系统内存占光，造成其他任务不能执行，这样就会出现很大的不确定性。Spark采用预申请内存资源，自己管理内存资源的方式来确保一个更稳定的内存环境。 数据存储、读取：具体数据的读写。ps: Spark定义数据存储的最小单元为Block。 数据寻址(单节点、集群中)：为了区分各Block，每个Block有唯一的标识Id(BlockId)。对于数据的查询，一方面，需要确定在具体哪个节点上（集群中寻址）；另一方面，需要确定在具体节点的内存中还是磁盘中，具体路径或引用是什么（节点中具体数据寻址）。 数据远程传输：Broadcast、Shuffle之类的操作涉及到数据的跨节点传输，所以需要有数据远程传输功能。 Spark中存储体系相关的类确定了Spark存储体系所提供的功能，我们再来看看Spark中存储体系相对应的具体的实现类。 BlockId: Block是Spark存储体系中，数据管理的基本单位；BlockId是数据块的唯一标识。 BlockInfo：Block的元数据信息，如存储Level(StorageLevel)等。 StorageLevel：定义了存储级别，内存还是磁盘，序列化还是非序列化，几个副本。 BlockInfoManager：使用一个Map来映射BlockId和其对应的BlockInfo; 另外维护了每个Block的读锁和写锁。 BlockResult：Block的数据结果，包括数据读取接口。 BlockManager：每个节点(driver、executor)中管理数据的接口，屏蔽了底层存储细节（内存、磁盘、序列化方式）。 BlockManagerId：Blockmanager的唯一标识。 BlockMangerMasterEndpoint：维护集群中所有的BlockManager及其维护的Block BlockManagerMaster：各BlockManager与 BlockMangerMasterEndpoint 通信的代理（RPC客户端）。 MapOutTracher：用于找到某Reduce对应的上游Block所在的位置。 BlockManagerSlaveEndpoint：各BlockManager中对外提供服务的RPC服务端，用于接收对该节点数据处理的请求（主要是删除数据）。 DiskStore：文件读写。 MemoryStore：内存读写管理，并根据存储级别将数据转存到磁盘等。 DiskBlockManager：资源申请，创建目录、文件等。 MemoryPool：预申请的内存空间，主要有四块，堆内执行内存、堆内存储内存、堆外执行内存和堆外存储内存。 MemoryManager：对预申请的内存进行分配与回收。目前有StaticMemoryManager 和 UnifiedMemoryManager 两种内存管理器，这里Static和Unified是相对执行内存和存储内存的关系来说的。Static即执行内存和存储内存各自空间相对是静态的、固定的；而Unified管理方式下对执行内存和存储内存统一管理，两者可以相互借用。 ShuffleClient：用于Block上传、下载的接口。 BlockTransferService：ShuffleClient底层的面向网络RPC的数据传输服务。 对这些类有个整体的认识，再去看源码应该会容易很多。 下面有两张图，比便于对Spark存储体系的整体认识，从网上直接粘过来的，把原文链接放在了参考资料里。 这张图跟《Spark内核设计的艺术》的一样，姑且作为出处： 这张图见参考资料： 参考资料： Spark存储体系","tags":[{"name":"spark","slug":"spark","permalink":"https://blog.guopengfei.top/tags/spark/"}]},{"title":"初探Flink之编程模型","date":"2019-05-13T02:03:53.000Z","path":"2019/05/13/Hello-Flink/","text":"我们从官网文档开始，一窥Flink初貌。(https://ci.apache.org/projects/flink/flink-docs-release-1.8/) 在学习Flink时，我们以和spark的对比，作为很重要的一条线来贯穿始终，通过对比两者的异同，来进一步理解分布式计算中需要解决的问题，及其解决方案。 Flink官网对其定义如下： Apache Flink is an open source platform for distributed stream and batch data processing.Flink’s core is a streaming dataflow engine that provides data distribution, communication, and fault tolerance for distributed computations over data streams. Flink builds batch processing on top of the streaming engine, overlaying native iteration support, managed memory, and program optimization. 从上述定义，我们了解到： Flink是一个开源的流处理和批处理平台； Flink的核心是流处理引擎； 批处理功能建立在流引擎之上。 与Spark异同 从功能定位来看，两者非常非常相似，都同时支持流计算和批处理； Spark 把流处理建立在批处理引擎上，流处理是微批处理计算； Flink把批处理建立在流处理引擎上，批处理是一个有限流。 官网文档给出了学习路径建议： 基础概念学习 (Dataflow Programming Model 和 Distributed Runtime Environment ) 学习导引 Implement and run a DataStream application Setup a local Flink cluster 下面我们就按照这个顺利来逐步深入，一窥究竟。 整体架构分层Flink 将整个架构分成如下四层： Stateful Stream Processing: 定义了最核心的任务模型和最底层处理函数； Core APIs : 定义了批处理和流处理相关的API； Table API：定义了一个更高层的面向Table 抽象的API，即数据有schema ，定义了Table上常用的各种操作(select，filter，group by)等； SQL ：SQL接口。 与Spark异同 整体结构两者非常类似； Flink中 流处理和批处理更加一致）; Spark中 基于RDD(最核心模型、批处理)创建了DataFrame（Table API）和Streaming(流处理)又分别在DataFrame和Streaming之上创建了 spark sql 和 struct streaming接口 而从上图可以很容易的看出flink对待流处理和批处理的一致性，ps: 阿里在致力于让两者更加一致。 Flink各层概念及定位更加清晰，可能更两者的发展有关，Spark是慢慢生长出来的，而Flink是阿里在其幼小时（2015年），大刀阔斧的改出来的，这个时期Spark已经是1.4版本（Spark和Flink 各 release版本见附录），其批处理、流处理及DataFrame已经比较完善。 ps: 只是初步印象，待深入研究结论未必如此。 编程模型 Flink的编程模型，主要是 Stream 及其上的 Transformations， 其中Stream 定义了分布式数据模型，Transformations 定义了 不同的Stream 所支持的各种操作（从上图也可以看出与Spark代码惊人的相似）。 与Spark异同 两者及其相似，核心都是分布式数据模型，及其上的操作组成； Spark 将操作分为 Transform(Lazy 操作，定义了数据模型的各种变换) 和 Action(真正触发 Job的执行)； 正如两者的核心理念不同，Spark基于批处理RDD模型，Flink 基于流处理 Stream模型。 执行时都会将代码生成 DAG（directed acyclic graphs）； 区分不同的操作依赖上游一个分区或多个分区的情况（Spark中窄依赖和宽依赖，Flink中One-to-one和Redistributing）。 思考：两者一个以流计算为核心，一个以批处理为核心，会是两者不可逾越的鸿沟吗？还是最终会渐渐一致？ 其他概念Windows和批处理不同的是，流处理面对的数据是无限的，所以不能对一个流的所有数据进行处理，因此需要将一个无限流划分出一些有限的窗口，对每个窗口内的数据进行计算。 可以按照时间(如每5秒)也可以按照数据量(如每500条数据)来划分窗口； Flink 定义了多种窗口类型以应对不同的业务需求，如 tumbling windows (无重叠), sliding windows (窗口间有重叠), and session windows (定义不活跃时间间隔来划分窗口). Time Flink中可以按照以下三种时间来进行数据对齐处理： Event Time： 事件时间，通常是日志中打的行为发生时的时间戳； Ingestion Time： 事件进入Flink 的时间； Processing Time： Flink任务执行时的本地时间。 Stateful Operations有些业务场景中，需要对多条Event记录进行汇总统计，而不是逐条记录单独处理。这种情况下的操作成为 Stateful(有状态)的。 在实现上，其实Flink维护了一个类似Key-Value的结构来记录各state。 Checkpoints 容错Flink 使用checkpoint和回放（stream replay）机制来做容错机制。 所谓checkpoint即定时的将数据的镜像进行持久化存储。当遇到故障时，从镜像点开始从新计算数据。 需要在checkpoint 保存周期和 故障时数据恢复时间之间做一个权衡。 批处理Flink 将批处理当成一种特殊的流(有限的流)，但Flink也有针对批处理的离线特性做一些优化： 批处理不适用 checkpoint 进行容错，遇到故障时从头（对应分区）计算数据； Stateful操作使用 in-memory/out-of-core数据结构，而不是使用 k-v 索引；(需要进一步研究) 有一些批处理独有的异步API。 附录一：Spark各版本发布时间 Version Original release date Latest version Release date 0.5 2012-06-12 0.5.1 2012-10-07 0.6 2012-10-14 0.6.2 2013-02-07[36] 0.7 2013-02-27 0.7.3 2013-07-16 0.8 2013-09-25 0.8.1 2013-12-19 0.9 2014-02-02 0.9.2 2014-07-23 1.0 2014-05-26 1.0.2 2014-08-05 1.1 2014-09-11 1.1.1 2014-11-26 1.2 2014-12-18 1.2.2 2015-04-17 1.3 2015-03-13 1.3.1 2015-04-17 1.4 2015-06-11 1.4.1 2015-07-15 1.5 2015-09-09 1.5.2 2015-11-09 1.6 2016-01-04 1.6.3 2016-11-07 附录二： Flink 各版本发布时间 Version Original release date Latest version Release date 0.9 2015-06-24 0.9.1 2015-09-01 0.10 2015-11-16 0.10.2 2016-02-11 1.0 2016-03-08 1.0.3 2016-05-11 1.1 2016-08-08 1.1.5 2017-03-22 1.2 2017-02-06 1.2.1 2017-04-26 1.3 2017-06-01 1.3.3 2018-03-15 1.4 2017-12-12 1.4.2 2018-03-08 1.5 2018-05-25 1.5.6 2018-12-26 1.6 2018-08-08 1.6.3 2018-12-22 1.7 2018-11-30 1.7.2 2019-02-15 1.8 2019-04-09 1.8.0 2019-04-09","tags":[{"name":"flink","slug":"flink","permalink":"https://blog.guopengfei.top/tags/flink/"}]},{"title":"spark源码解析alpha-0.1","date":"2019-02-15T02:26:07.000Z","path":"2019/02/15/spark源码解析alpha-0-1RDD与任务调度/","text":"Spark源码：https://github.com/apache/spark.git 版本号：alpha-0.1 Spark最核心的概念是RDD（分布式弹性数据集）数据模型，在alpha-0.1版本中实现了RDD数据模型及在其上的任务调度系统，另外还实现了Broadcast和Accumulator工具以及SparkContext接口和spark-shell接口，下面对其分别做介绍。 RDD与任务调度​ 百度百科对RDD介绍如下：RDD(Resilient Distributed Datasets)，弹性分布式数据集，是分布式内存的一个抽象概念。RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，只能通过在其他RDD执行确定的转换操作（如map、join和group by）而创建，然而这些限制使得实现容错的开销很低。​ ​ 我们可以将RDD看成一个链表，链表的每个节点在上一个节点数据的基础上增加了一些操作而生成。计算最终数据时，只需按照链表上的操作顺序对数据进行计算即可。 data -&gt;f1(data)-&gt;f2(f1(data))……-&gt;fn(…f1(data)…) RDD将数据分成多个split，每个split可以单独形成操作链表，及整个RDD成为了一组链表。 不同的操作产生不同的RDD即形成了RDD的类体系。 RDD的APIRDD的函数可以分为两类，一类是用户接口算子，包括对数据进行操作的transform算子，如map、filter、reduce等和触发任务执行的Action算子如 count、collect、foreach等；另一类是任务执行时需要的函数，如split、iterator等，子类通过复写这些函数来实现不同的子RDD。在alpha-0.1版本中RDD实现了以下函数: 执行函数（子类重载） def splits: Array[Split] 获取数据分片 def iterator(split: Split): Iterator[T] 数据分片上的迭代器 def preferredLocations(split: Split): Seq[String] 数据分片引用的数据地址 def taskStarted(split: Split, slot: SlaveOffer) 任务是否启动 Transform算子 def map(f: T =&gt; U):MappedRDD def filter(f: T =&gt; Boolean):FilteredRDD def aggregateSplit():SplitRDD def cache():CachedRDD def def sample(withReplacement: Boolean, frac: Double,seed: Int):SampledRDD def flatMap(f: T =&gt; Traversable[U]):FlatMappedRDD Action算子 def foreach(f: T =&gt; Unit):Unit def collect(): Array[T] def toArray(): Array[T] def reduce(f: (T, T) =&gt; T): T def take(num: Int): Array[T] def first: T def count(): Long def union(other: RDD[T]):UnionRDD def ++(other: RDD[T]):UnionRDD def cartesian(other: RDD[U]):CartesianRDD RDD类体系RDD分为两大类，一类根据外部数据生成，是RDD的起点，在alpha-0.1版本中只有HdfsTextFile 和ParallelArray；另一类是在RDD增加操作产生，如 MappedRDD、FilteredRDD等，该类RDD和transform算子相对应。继承体系如下： alpha-0.1中实现的RDD有： HdfsTextFile(sc: SparkContext, path: String) ParallelArray(sc: SparkContext, data: Seq[T], numSlices: Int) MappedRDD(prev: RDD[T], f: T =&gt; U) FilteredRDD(prev: RDD[T], f: T =&gt; Boolean) FlatMappedRDD(prev: RDD[T], f: T =&gt; Traversable[U]) SplitRDD(prev: RDD[T]) SampledRDD(prev: RDD[T], withReplacement: Boolean, frac: Double, seed: Int) CachedRDD(prev: RDD[T]) UnionRDD(sc: SparkContext, rdd1: RDD[T], rdd2: RDD[T]) CartesianRDD(sc: SparkContext, rdd1: RDD[T], rdd2: RDD[U]) 任务调度如上所述，Action算子会产生任务，并触发任务的提交。下面我们以foreach为例，追踪任务调度流程。 在action算子（foreach）中，对每个分区(split)生成Task（ForeachTask）实例，并调用sc（SparkContext）中的 runTaskObjects函数来执行任务。 12345def foreach(f: T =&gt; Unit) &#123; val cleanF = sc.clean(f) val tasks = splits.map(s =&gt; new ForeachTask(this, s, cleanF)).toArray sc.runTaskObjects(tasks)&#125; 2、在SparkContext的 runTaskObjects函数中，调用 Scheduler实例的 runTasks函数来执行任务。 1234567891011class SparkContext(master: String, frameworkName: String) extends Logging &#123; private[spark] def runTaskObjects[T: ClassManifest](tasks: Seq[Task[T]]) : Array[T] = &#123; logInfo(\"Running \" + tasks.length + \" tasks in parallel\") val start = System.nanoTime val result = scheduler.runTasks(tasks.toArray) logInfo(\"Tasks finished in \" + (System.nanoTime - start) / 1e9 + \" s\") return result &#125;&#125; Scheduler与Task类体系 SparkContextSparkContext有两大职责，一方面管理着spark运行所需的环境，在alpha-0.1中主要是 任务调度器Scheduler；另一方面向用户提供了编程API。主要函数如下： 运行环境 scheduler: Scheduler 任务调度器 def runTasks(tasks: Array[() =&gt; T]): Array[T] 任务执行函数（由rdd的action算子调用） 编程API def textFile(path: String) : HdfsTextFile def parallelize(seq: Seq[T], numSlices: Int):ParallelArray[T] def parallelize(seq: Seq[T]):ParallelArray[T] def accumulator():Accumulator def broadcast(value: T):CentralizedHDFSBroadcast 参考文献 Spark-alpha-0.1源码解读：https://www.jianshu.com/p/795302f94fa1 百度百科：https://baike.baidu.com/item/RDD/5840158","tags":[{"name":"spark","slug":"spark","permalink":"https://blog.guopengfei.top/tags/spark/"},{"name":"源码","slug":"源码","permalink":"https://blog.guopengfei.top/tags/源码/"}]},{"title":"SparkStreaming反压机制详解","date":"2019-01-04T04:24:03.000Z","path":"2019/01/04/SparkStreaming反压机制详解/","text":"背景概念 DStream： 表示一系列时间序列上连续的RDDs。 Batch Duration：spark streaming的核心参数，设置流数据被分成多个batch的时间间隔，每个spark引擎处理的就是这个时间间隔内的数据。 InputDStream：InputDStream继承自DStream，是所有输入流的基类，代表从源接收到的原始数据流DStreams，每一个InputDStream关联到单个Receiver对象，从源数据接收数据并存储到spark内存，等待处理。 反压是什么反压可以限制每个batch接收到的消息量 Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。为什么设置反压如果在一个batch内收到的消息过多，这就需要为executor分配更多内存，可能会导致其他spark streaming应用程序资源分配不足，甚至有OOM的风险。反压机制就可以动态控制batch接收消息速率，来适配集群处理能力。 设置 开启反压 SparkConf.set(“spark.streaming.backpressure.enabled”, “true”) 设置每个kafka partition读取消息的最大速率: SparkConf.set(“spark.streaming.kafka.maxRatePerPartition”, “spark.streaming.kafka.maxRatePerPartition”) 这个值要结合spark Streaming处理消息的速率和batchDuration， 尽量保证读取的每个partition数据在batchDuration时间内处理完， 这个参数需要不断调整，以做到尽可能高的吞吐量.基本原理速率预估 rateController （in DirectKafkaInputDStream） RateController : 继承自StreamingListener. 用于处理BatchCompleted事件(见下图)。 RateEstimator PIDRateEstimator 限流maxMessagesPerPartition (in DirectKafkaInputDStream.scala) com.google.common.util.concurrent.RateLimiter RateLimiter是guava提供的基于令牌桶算法的实现类，可以非常简单的完成限流特技，并且根据系统的实际情况来调整生成token的速率。 需要注意的坑从多个Topic创建Stream根据 maxMessagesPerPartition 代码可知，每次根据上一个速率来预估速率，如果多个topic速率相差过大，会造成预估的速率忽大忽小(如下图)，速率很不稳定，整体上接近多个速率的平均值。 一个Job多个Stream由于只能配置一个参数，所以只能配置一个各topic对应partion速率的折中值，若不同topic的partition速率差别太大，则很难两全。 ps:按小的设，大的会频繁甚至始终命中背压。按大的设，小的起不到背压效果而OOM。maxRatePerPartition参数设置这个是速率，不需要乘以batch duration了。 参考文献 再谈Spark Streaming Kafka反压 https://www.jianshu.com/p/c0b724137416 Spark Streaming性能优化: 如何在生产环境下应对流数据峰值巨变 https://www.cnblogs.com/itboys/p/6486089.html","tags":[{"name":"spark","slug":"spark","permalink":"https://blog.guopengfei.top/tags/spark/"}]}]